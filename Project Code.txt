ssh USER@IP_ADDRESS --Remember to change USER with your actual username and IP_ADDRESS to the respective IP of your server/cluster.

curl 'https://csula-my.sharepoint.com/personal/mbates4_calstatela_edu/_layouts/15/download.aspx?SourceUrl=%2Fpersonal%2Fmbates4%5Fcalstatela%5Fedu%2FDocuments%2FCollege%20Semester%208%2FCIS4560%2FProject%2FURWarTweetsMar31%2Ezip' \
  -H 'authority: csula-my.sharepoint.com' \
  -H 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9' \
  -H 'accept-language: en-US,en;q=0.9' \
  -H 'cookie: MicrosoftApplicationsTelemetryDeviceId=433cea18-1a64-7c3f-9ffb-61bf77de5578; MicrosoftApplicationsTelemetryFirstLaunchTime=1651476162187; PowerPointWacDataCenter=US1; WordWacDataCenter=PUS5; WacDataCenter=PUS5; CCSInfo=NS8xMS8yMDIyIDEyOjIyOjE2IEFNEHfR+kM3cYk5vKWTyKV9MbqAKVUUUHc0u634zpmoM6+Q61ei3CUfgE+R5OZOGE8g4oaRlgF8QLSHWjk/Ai+QC8/xRBj5u/0S0yXSyBmyHeb1Xskg2Ck1mTN1u+zcR6X9VXXIv9xEnMj6bGqRAQ+uIHIWP+Wpykyy8Ha1K0bcFU5Q/cH9i4VRN+Mii0ghWkAM1Bg5ZPO9L7MqBcvBGVVqNl+h3YdmMexc5b0fZu83WhCh+QDJFqewXHsUbBkOZGhfuP3K1s94OiJ6b2i+FZxf7fulNA8+3xmPz7PMpdq10+CtgQPSl4dksTZ1cznYQW/qJc4ZGyLu2gwexyZdoj7V7RUAAAA=; KillSwitchOverrides_enableKillSwitches=; KillSwitchOverrides_disableKillSwitches=; rtFa=iXQu/6V7+nmJhJHmQGcqY8qQjGR711khdm1fPI/qg7UmMUIxMDA4ODktNEE3Qi00REMyLUFFRDItRDQ4QjkwQzcyQ0Q1IzEzMjk2NDU2NzIxNDI3OTE4OSNBRDQwM0JBMC02MDNCLUMwMDAtRTIxMC1EQTg4QjcyOTBCNzkjTUJBVEVTNCU0MENBTFNUQVRFTEEuRURVabqhf7mNHQqWD6c1eMRKVWGU770xdkfY+sNxxOTjBWiHxxFoSYixEoMnNKc9ng6XQnWQ5El0V3TGB0xPE4uucZ1LsivAfkF0LnViE8Tgie2SJpdSEoghAUw5Ed4u0sZ1EaUqpEHNnP62oThZXkCCarKAvG61IMM6a4shtqUde2VYdYU1+eU2G0PPNl3vEOrgLvuQ8UG6UuAuSXmspB6E/m+bpF/9u47Ep0yMc71jHa1rW6lpL0Xi1M87LNonsEfCP8JGwdgdFZ8wSFG71SaUwETYM0arxX1Zs/4K3ig9GWb0G/M/h6ZXJQT6L2p6rHiz0BzltW/NphysvD9XP1Qq/ZYAAAA=; FedAuth=77u/PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz48U1A+VjEyLDBoLmZ8bWVtYmVyc2hpcHwxMDAzM2ZmZmE3MzU2N2JmQGxpdmUuY29tLDAjLmZ8bWVtYmVyc2hpcHxtYmF0ZXM0QGNhbHN0YXRlbGEuZWR1LDEzMjk2NDU2NzEyMDAwMDAwMCwxMzE2ODA1NjI5MjAwMDAwMDAsMTMyOTY4ODg3MjE0MTIyOTQ3LDY0LjUyLjE3Ni4xMDYsMywxYjEwMDg4OS00YTdiLTRkYzItYWVkMi1kNDhiOTBjNzJjZDUsLGIwZDdlYmRlLTYwODQtNGI0OS1hMTA0LWJlNDFlMmQxZDNmZixhZDQwM2JhMC02MDNiLWMwMDAtZTIxMC1kYTg4YjcyOTBiNzksYWQ0MDNiYTAtNjAzYi1jMDAwLWUyMTAtZGE4OGI3MjkwYjc5LCwwLDEzMjk2NDYwMzIxMzY1NDI0NywxMzI5NjcxNTkyMTM2NTQyNDcsLCxleUo0YlhOZlkyTWlPaUpiWENKRFVERmNJbDBpTENKNGJYTmZjM050SWpvaU1TSXNJbkJ5WldabGNuSmxaRjkxYzJWeWJtRnRaU0k2SW0xaVlYUmxjelJBWTJGc2MzUmhkR1ZzWVM1bFpIVWlMQ0oxZEdraU9pSlVWMWw1UVVaU1gyTXdZVkpzVkRrNFVFbEtNMEZCSW4wPSwyNjUwNDY3NzQzOTk5OTk5OTk5LDEzMjk2NDU2NzIxMDAwMDAwMCxmNzY4YTE0MS00YzkwLTQwODgtODQ0Ny0yOTEyNjBkYzZiN2MsLCwsLCwwLCxvc3krbnAzM3M0aDN4VnE1WEM4QzZ3ZDR1SDNjWkVEcm5rOXpxZlpVSVdrdTFHcEpWM2h5clFzaG1uTW4yR3ZZKzFaWTFwTkNyZHBhRVJFeUJpZ0dFdFM4Rm1PajRNSnVBOHZTOTNFTUN0ZUp6Vng1Y2JyYWRBKzdrWW1uZ2Y4MW9yR2puRlZ3NnJ2MmZtOUcwYmdBdXJ1NmxvU2NCcXgzUmVXTTBKeU9vNVdTNHdmRDFlcWpNZWw4ZVhzWnYwTmNsaHBNWWg3REZzbUl4aVBXMVNYQVJKbEtwMlFEQ25QYW8vWmFyQnJuVmFaRzBPVGpmZDl0QytVeXg1UlVNY0VFQVJXamJqWnVxWlBaM3ZFRkorc0E3cUpCV2dHYXA3NFVWbFdDWWFqQXAvcXErZkM5MTdRV0Y5UEhyTnJqUmtlU20xN3AwT2NxTUFydk5rMC9XRkE1bFE9PTwvU1A+; cucg=1' \
  -H 'dnt: 1' \
  -H 'referer: https://csula-my.sharepoint.com/personal/mbates4_calstatela_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fmbates4%5Fcalstatela%5Fedu%2FDocuments%2FCollege%20Semester%208%2FCIS4560%2FProject%2FURWarTweetsMar31%2Ezip&parent=%2Fpersonal%2Fmbates4%5Fcalstatela%5Fedu%2FDocuments%2FCollege%20Semester%208%2FCIS4560%2FProject' \
  -H 'sec-ch-ua: " Not A;Brand";v="99", "Chromium";v="101", "Google Chrome";v="101"' \
  -H 'sec-ch-ua-mobile: ?0' \
  -H 'sec-ch-ua-platform: "Linux"' \
  -H 'sec-fetch-dest: iframe' \
  -H 'sec-fetch-mode: navigate' \
  -H 'sec-fetch-site: same-origin' \
  -H 'sec-gpc: 1' \
  -H 'service-worker-navigation-preload: true' \
  -H 'upgrade-insecure-requests: 1' \
  -H 'user-agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36' \
  --compressed --output War_Tweets.zip

wget -O dictionary.tsv https://github.com/dalgual/aidatasci/raw/master/data/bigdata/dictionary.tsv

unzip War_Tweets.zip

ls (To ensure the file decompressed properly and the CSV file is there)

rm War_Tweets.zip (To save space)

hdfs dfs -mkdir tmp

hdfs dfs -mkdir tmp/data

hdfs dfs -mkdir tmp/data/tables

hdfs dfs -mkdir tmp/data/tables/dictionary

hdfs dfs -mkdir tmp/data/prep

hdfs dfs -put CombinedMar31.csv tmp/data/prep

hdfs dfs -ls tmp/data/prep (Ensure the files are in Hadoop)

rm CombinedMar31.csv (To save Linux server space)

hdfs dfs -put dictionary.tsv tmp/data/tables/dictionary/

hdfs dfs -ls tmp/data/tables/dictionary/ (Ensure the files are in Hadoop)

rm dictionary.tsv (To save Linux server space)

beeline

use (your database);

CREATE EXTERNAL TABLE IF NOT EXISTS twitterdata(userid BIGINT,username STRING,acctdesc STRING,location STRING,follows BIGINT,followers BIGINT,totaltweets BIGINT,usercreatedts STRING,tweetid BIGINT,tweetcreatedts STRING,retweetcount BIGINT,text STRING,language STRING,favorite_count BIGINT,extractedts STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'tmp/data/prep/' TBLPROPERTIES('skip.header.line.count'='1'); -- Create external table, inserting data from the cleaned and sampled datafile.

show tables; (To ensure table was created)

SELECT * FROM twitterdata LIMIT 1; (To ensure the data exists in the table)

--Taking the time to do some basic SQL analysis
SELECT language, COUNT(language) FROM twitterdata WHERE language IS NOT NULL GROUP BY language LIMIT 500; --Let's see what language composes the majority of these 500 tweets. (Make note that en "English" has significantly more records than every other language within the first 500 tweets.)

--Insert additional queries here

CREATE TABLE TrimmedTweets AS SELECT tweetid, text, language, tweetcreatedts FROM twitterdata WHERE language='en'; --We'll create a new table containing only the releveant and needed data written in the english language so we can use it for sentiment analysis later against our dictionary file.

SHOW tables; --Check to see if the table was created.

SELECT * FROM trimmedtweets LIMIT 50; --Ensure that the data was inserted properly into the new table.

CREATE EXTERNAL TABLE if not exists dictionary (type string, length int, word string, pos string, stemmed string, polarity string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE LOCATION 'tmp/data/tables/dictionary'; --Create dictionary table using the dictionary file in hadoop.

SHOW tables; --Ensure table was created successfully

--Begin Hive Sentiment Analysis by more or less following Lab 4 tutorial

CREATE VIEW IF NOT EXISTS levelOne AS SELECT tweetid, words FROM trimmedtweets LATERAL VIEW EXPLODE(SENTENCES(lower(text))) DUMMY AS words; --Create level 1 view to break down the structure of the tweets (taken from Professor Woo's Lab 4).

CREATE VIEW IF NOT EXISTS levelTwo AS SELECT tweetid, word FROM levelOne LATERAL VIEW EXPLODE(words) DUMMY AS word; --Create level 2 view to break down tweet even further into individual words (taken from Professor Woo's Lab 4).

CREATE VIEW IF NOT EXISTS levelThree AS SELECT tweetid, levelTwo.word, case d.polarity when 'negative' then -1 when 'positive' then 1 else 0 end as polarity FROM levelTwo LEFT OUTER JOIN dictionary d on levelTwo.word = d.word; --Create final level 3 which actually compared words against dictionary table to determine sentiment

SELECT * FROM levelThree LIMIT 5; --Check to ensure that the queries executed correctly and the view contains the correct information

CREATE TABLE IF NOT EXISTS tweetSentiment STORED AS ORC AS SELECT tweetid, CASE WHEN SUM(polarity) > 0 THEN 'positive' WHEN SUM(polarity) < 0 THEN 'negative' ELSE 'neutral' END AS sentiment FROM levelThree GROUP BY tweetid; --Take the level 3 view and convert it to a table, take the numeric values of the sentiment in the level 3 table and turn them into readable positive, negative, and neutral string values

CREATE TABLE IF NOT EXISTS sentimentProc ROW FORMAT DELIMITED FIELDS TERMINATED BY "," STORED AS TEXTFILE LOCATION "tmp/sentimentproc" AS SELECT t.*, CASE s.sentiment WHEN 'positive' THEN 2 WHEN 'neutral' THEN 1 WHEN 'negative' THEN 0 END AS sentiment FROM trimmedtweets t LEFT OUTER JOIN tweetSentiment s ON t.tweetid = s.tweetid; -- Create another table to a file that combined trimmed twitter data and adds the sentiment as a column in a numerical value as show in the lab 4 tutorial

SELECT * FROM sentimentproc LIMIT 3; --Ensure the table was created correctly and the data is there

SELECT COUNT(*) FROM sentimentproc; --To find the number of rows in our semi-finished table

CREATE TABLE IF NOT EXISTS finaltweetsent ROW FORMAT DELIMITED FIELDS TERMINATED BY "," STORED AS TEXTFILE LOCATION "tmp/finaltweetsent" AS SELECT tweetid, REGEXP_REPLACE(text, '[^a-zA-Z0-9]+', ' ') tweet, language, tweetcreatedts, sentiment FROM sentimentproc WHERE tweetid IS NOT NULL AND (sentiment =0 OR sentiment =1 OR sentiment =2) AND 1=0 LIMIT 1; --Creating the final table to a file and removing any non-alphabetical and non-numeric characters from the text column for visual analysis purposes

INSERT INTO TABLE finaltweetsent SELECT tweetid, REGEXP_REPLACE(text, '[^a-zA-Z0-9]+', ' ') tweet, language, tweetcreatedts, sentiment FROM sentimentProc WHERE tweetid IS NOT NULL AND (sentiment =0 OR sentiment =1 OR sentiment =2); --Inserting data into previously created table

show tables; --Ensure table exists after creation

SELECT * FROM finaltweetsent LIMIT 10; --Check table for data

-- Download file to local machine 

hdfs dfs -ls tmp/finaltweetsent --Should see several files

hdfs dfs -get tmp/finaltweetsent/00000*_0 --This will put the 7 output files from HDFS into the Linux server

ls -al --Ensure that all 7 files were downloaded to the Linux Server from HDFS

cat 000000_0 000001_0 000002_0 000003_0 000004_0 000005_0 000006_0 > sentiment_out.csv --Combine the numerous output files into a single .csv file 

ls -al --Ensure the file was created successfully

rm 00000*_0 --remove excess output files to save linux server space

scp USER@IP_ADDRESS:/home/USER/sentiment_out.csv 000000_0.csv --From your local machine, download the file. Remember to replace USER with your username IP_ADDRESS to the respective IP of your server/cluster.